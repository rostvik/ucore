{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"uCore","text":"<p>This setup builds on top of the HCI tag to bring extra features to the OS.</p> <ul> <li>Komodo, a service to bring some of the joys of gitops to a docker/podman compose setup</li> <li>1Password, using op (onepassword cli) to inject secrets into config files and services</li> </ul>"},{"location":"bootstrap/","title":"Bootstraping the box","text":""},{"location":"bootstrap/#enroll-ublue-secure-boot-key","title":"Enroll uBlue Secure Boot key","text":"<p>To use TPM based LUKS, we need to get secure boot going</p> <pre><code>sudo mokutil --import /etc/pki/akmods/certs/akmods-ublue.der\n</code></pre> <p>The ZFS module is signed with uBlues key.</p> <p>After the the cert is imported, reboot the machine, and select to add the MOK in the blue screen popping up.</p> <p>As this is using the shim, the Microsoft SB keys can now be used.</p>"},{"location":"bootstrap/#running-ucore","title":"Running uCore","text":"<p>To prime the secrets needed, and start the required services, a convenience script is created at <code>/etc/usr/sbin/bootstrap.sh</code></p> bootstrap.sh<pre><code>#!/usr/bin/env bash\n\nset -e\n\nEDITOR=vim\n\n# Prompt for editing of service to set up 1password cli\nsudo systemctl edit komodo-seed-config.service\n\n# Enable and run the service responsible for seeding the config files\nsudo systemctl enable --now komodo-seed-config.service\n\n# Enable Podman services\nsudo systemctl enable --now podman.socket\nsystemctl enable --now --user podman.socket\n\n# Enable Komondo backend services\nsudo systemctl enable komodo-periphery.service\nsystemctl enable --user komodo-periphery.service\n\n# Enable and log in to Tailscale\nsudo systemctl enable --now tailscaled.service\nsudo tailscale login\n\n# Reboot for services to take effect in order\nsudo systemctl reboot\n</code></pre>"},{"location":"komodo/","title":"Komodo","text":"<p>This repo sets up Systemd services for the Komodo backend in both rootless and rootful mode, represented in the UI by separate servers.</p>"},{"location":"komodo/#database","title":"Database","text":"<p>Komodo is running with the default database choice, MongoDB. This runs as a Quadlet<sup>1</sup></p> komodo-mongo.container<pre><code>[Unit]\nDescription=MongoDB for Komodo\nAfter=komodo-seed-config.service\nRequires=komodo-seed-config.service\n\n[Container]\nImage=docker.io/library/mongo:4\n\n# Use volume and network defined below\nVolume=/opt/docker/komodo/mongo:/data/db:z\nVolume=/etc/passwd:/etc/passwd:ro\nNetwork=komodo-backend.network\n\nContainerName=komodo-mongo\nLogDriver=journald\n\nLabel=komodo.skip\nExec=--quiet --wiredTigerCacheSizeGB 0.25\n\nPodmanArgs=--memory 5g\nPodmanArgs=--cpus 2\n\nNoNewPrivileges=true\nReadOnly=true\nReadOnlyTmpfs=true\nUser=1000\nGroup=1000\n\n[Service]\n# Restart service when sleep finishes\nRestart=always\n# Extend Timeout to allow time to pull the image\nTimeoutStartSec=900\n\n[Install]\n# Start by default on boot\nWantedBy=multi-user.target default.target komodo-core.service\n</code></pre> <p>The <code>komodo-seed-config.service</code> is responsible for setting up the username and password by templating to the file <code>/etc/containers/systemd/komodo-mongo.container.d/override.conf</code></p> komodo-mongo.container.d/override.conf<pre><code>[Container]\nEnvironment=MONGO_INITDB_ROOT_USERNAME=op://Home-infra/uCore - komodo/database.user\nEnvironment=MONGO_INITDB_ROOT_PASSWORD=op://Home-infra/uCore - komodo/database.password\n</code></pre>"},{"location":"komodo/#periphery","title":"Periphery","text":"<p>Periphery is the server component of Komodo, this runs on each server you want to manage in your \"fleet\".</p> <p>It can be set up in a couple of ways, directly as a container, or with the binary running straight on the host. I opted for the latter, as it becomes less cumbersome (in my head) to make it work with Podman.</p> SystemUser komodo-periphery.service<pre><code>[Unit]\n# Based on https://github.com/moghtech/komodo/blob/main/scripts/setup-periphery.py\nDescription=Agent to connect with Komodo Core\nAfter=komodo-seed-config.service\nRequires=komodo-seed-config.service\n\n[Service]\nEnvironment=PERIPHERY_PORT=8120\nEnvironment=DOCKER_HOST=unix:///run/podman/podman.sock\nEnvironment=PERIPHERY_SSL_KEY_FILE=/opt/docker/komodo/periphery-root/key.pem\nEnvironment=PERIPHERY_SSL_CERT_FILE=/opt/docker/komodo/periphery-root/cert.pem\nEnvironment=PERIPHERY_ROOT_DIRECTORY=/opt/docker/komodo/periphery-root\nExecStart=/usr/bin/periphery --config-path /opt/docker/komodo/periphery.config.toml\nRestart=on-failure\nTimeoutStartSec=0\n\n[Install]\nWantedBy=default.target komodo-mongo.service\n</code></pre> komodo-periphery.service<pre><code>[Unit]\n# Based on https://github.com/moghtech/komodo/blob/main/scripts/setup-periphery.py\nDescription=User agent to connect with Komodo Core\n\n[Service]\nEnvironment=PERIPHERY_PORT=8121\nEnvironment=DOCKER_HOST=unix:///var/run/user/1000/podman/podman.sock\nEnvironment=PERIPHERY_SSL_KEY_FILE=/opt/docker/komodo/periphery-user/key.pem\nEnvironment=PERIPHERY_SSL_CERT_FILE=/opt/docker/komodo/periphery-user/cert.pem\nEnvironment=PERIPHERY_ROOT_DIRECTORY=/opt/docker/komodo/periphery-user\nExecStart=/usr/bin/periphery --config-path /opt/docker/komodo/periphery.config.toml\nRestart=on-failure\nTimeoutStartSec=0\n\n[Install]\nWantedBy=default.target\n</code></pre> <p>Periphery is set up as two services, system and user. This is why the service file mixes both file and environment variable configuration, where the environment is set to handle the differences between the services, like the port, paths and podman socket location.</p> <p>This is also set up to depend on <code>komodo-seed-config.service</code> to support potential further secrets in the config file.</p>"},{"location":"komodo/#core","title":"Core","text":"<p>The Core is the component that ties it all together, providing the UI as well as the logic. This also runs as a Quadlet.</p> komodo-core.container<pre><code>[Unit]\nDescription=Komodo core\nAfter=komodo-seed-config.service\nRequires=komodo-seed-config.service\n\n[Container]\nImage=ghcr.io/moghtech/komodo-core:latest\n\n# Use volume and network defined below\nVolume=repo-cache:/repo-cache\nVolume=/opt/docker/komodo/core.config.toml:/config/config.toml:ro,Z\n\nNetwork=komodo-backend.network\nPublishPort=9120:9120\n\nContainerName=komodo-core\nLogDriver=journald\n\nLabel=komodo.skip\n\nPodmanArgs=--memory 5g\nPodmanArgs=--cpus 2\n\nNoNewPrivileges=true\nReadOnly=true\nReadOnlyTmpfs=true\n\n[Service]\n# Restart service when sleep finishes\nRestart=always\n# Extend Timeout to allow time to pull the image\nTimeoutStartSec=900\n\n[Install]\n# Start by default on boot\nWantedBy=multi-user.target default.target\n</code></pre> <p>The same <code>komodo-seed-config.service</code> is set as a dependency, to template out the core config file, telling it about the database and other settings that you might hide.</p> <ol> <li> <p>Quadlet is a way to define, manage and run a podman container as a systemd service.\u00a0\u21a9</p> </li> </ol>"},{"location":"shell/","title":"Shell","text":"<p>This image installs ZSH</p>"},{"location":"shell/#dotfiles","title":"Dotfiles","text":"<p>This image has Chezmoi, as well as any binaries Roxedus/dotfiles would usually place in <code>~/.local/bin</code>.</p> <p>This is a minimal, recommended chezmoi config to prevent Chezmoi to manage these binaries.</p> .config/chezmoi/chezmoi.yml<pre><code>data:\n  integration:\n    bitwarden:\n      enabled: false\n    eza:\n      managed: false\n    hishtory:\n      managed: false\n    oh_my_posh:\n      managed: false\n</code></pre>"},{"location":"storage/","title":"Storage","text":""},{"location":"storage/#zfs-pool","title":"ZFS Pool","text":"<p>The target machine has 2 NVME drives intended to serve a mirror.</p> <p>Find disks:</p> <pre><code>sudo lsblk -o NAME,SIZE,SERIAL,LABEL,FSTYPE\n</code></pre> <p>Create pool:</p> <pre><code>sudo zpool create \\\n  -o ashift=12 \\  # (1)\n  -O aclinherit=passthrough \\\n  -O acltype=posix \\\n  -O atime=off \\ # (2)\n  -O compression=lz4 \\\n  -O xattr=sa \\ # (3)\n  -m none \\ # (4)\n  tank \\ # (5)\n  mirror \\\n  /dev/disk/by-id/nvme-KINGSTON_SFYRD2000G_50026B768694E467 \\\n  /dev/disk/by-id/nvme-KINGSTON_SFYRD2000G_50026B768694EB03\n</code></pre> <ol> <li>Disk offers 4k sectors, same as here</li> <li>Updates the access time on read, not useful data, set to false to prevent those writes.</li> <li>This machine has SELinux enabled, and setting this to <code>sa</code> is therefore recommended.</li> <li>Mounting the pool itself isn't desired with the dataset layout, those will be mounted later.</li> <li>Name of the pool</li> </ol> <p>Create datasets:</p> <pre><code>sudo zfs create -o encryption=on -o keyformat=raw -o keylocation=file:///run/zfs-hex -o mountpoint=/mnt/spicy tank/spicy\n</code></pre>"}]}